{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa22828",
   "metadata": {},
   "source": [
    "### REI505M Final project: Music genre classification starter pack\n",
    "\n",
    "The following Dataset class operates on the GTZAN dataset.\n",
    "\n",
    "* The duration of most GTZAN files are 30 seconds (3022050=661500 samples) but some are slightly shorter (approx 29.9 seconds). For this reason we truncate at 660000 samples below.\n",
    "* It may be beneficial to work with smaller chunks than ~30 seconds.\n",
    "* You may want to perform the data augmentations in the `__get_item__` function.\n",
    "* For now, `train_dataset` contains all the dataset, you need to set aside some examples for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "\n",
    "audio_dir = 'music/' # Path to folder with GTZAN files\n",
    "# music/\n",
    "#  - rock/\n",
    "#       rock.00099.wav\n",
    "#       ...\n",
    "#  - reggie/\n",
    "#  ...\n",
    "#  - blues/\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, audio_path,\n",
    "                 maxlen, sampling_rate, duration):\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_path = audio_path\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.duration = duration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        audio_file = self.audio_files[idx]\n",
    "        audio_dir = audio_file[:audio_file.index('.')]\n",
    "        file_path = os.path.join(self.audio_path, audio_dir, audio_file)\n",
    "        (rate,audio_samples) = wav.read(file_path)\n",
    "        audio_samples = torch.from_numpy(audio_samples).to(torch.float32)\n",
    "        if len(audio_samples) > self.maxlen:\n",
    "            # Truncate\n",
    "            audio_samples = audio_samples[:self.maxlen]\n",
    "\n",
    "        tstart = 0 # Offset from start of song (hyper-parameter!)\n",
    "        audio_samples = audio_samples[int(self.sampling_rate*tstart):int(self.sampling_rate*(tstart+self.duration))]\n",
    "\n",
    "        return audio_samples, label\n",
    "\n",
    "label_map={'blues' : 0, 'classical' : 1, 'country' : 2,\n",
    "           'disco' : 3, 'hiphop'    : 4, 'jazz'    : 5,\n",
    "           'metal' : 6, 'pop'       : 7, 'reggae'  : 8, 'rock' : 9}\n",
    "\n",
    "audio_files = []\n",
    "labels = []\n",
    "for root, subdirs, files in os.walk(audio_dir):\n",
    "    for fname in files:\n",
    "        if fname == '.DS_Store':\n",
    "            continue\n",
    "        audio_files.append(fname)\n",
    "        labels.append(label_map[fname[:fname.index('.')]])\n",
    "\n",
    "torch.manual_seed(0) # Reproducible results\n",
    "\n",
    "# TODO: Create validation and test sets\n",
    "train_files = [audio_files[i] for i in range(len(audio_files))]\n",
    "train_labels = [labels[i] for i in range(len(audio_files))]\n",
    "print(\"Training set:\", len(train_files))\n",
    "#print(\"Validation set:\", len(validation_files))\n",
    "#print(\"Test set:\", len(test_files))\n",
    "#assert len(set(train_files) & set(validation_files) & set(test_files)) == 0\n",
    "\n",
    "train_dataset = AudioDataset(audio_files=train_files, labels=train_labels,\n",
    "                             audio_path=audio_dir, \n",
    "                             maxlen=660000, sampling_rate=22050, duration=25)\n",
    "test_dataset=AudioDataset(audio_files=test_files, labels=test_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25)\n",
    "val_dataset=AudioDataset(audio_files=validation_files, labels=validation_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False )\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "tmp_features, tmp_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
