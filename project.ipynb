{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa22828",
   "metadata": {},
   "source": [
    "### REI505M Final project: Music genre classification starter pack\n",
    "\n",
    "The following Dataset class operates on the GTZAN dataset.\n",
    "\n",
    "* The duration of most GTZAN files are 30 seconds (3022050=661500 samples) but some are slightly shorter (approx 29.9 seconds). For this reason we truncate at 660000 samples below.\n",
    "* It may be beneficial to work with smaller chunks than ~30 seconds.\n",
    "* You may want to perform the data augmentations in the `__get_item__` function.\n",
    "* For now, `train_dataset` contains all the dataset, you need to set aside some examples for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891d2051",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "audio_dir = 'music/' # Path to folder with GTZAN files\n",
    "# music/\n",
    "#  - rock/\n",
    "#       rock.00099.wav\n",
    "#       ...\n",
    "#  - reggie/\n",
    "#  ...\n",
    "#  - blues/\n",
    "\n",
    "batch_size = 32\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, audio_path,\n",
    "                 maxlen, sampling_rate, duration, augment=False):\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_path = audio_path\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.duration = duration          # seconds\n",
    "        self.augment = augment\n",
    "\n",
    "        self.target_len = int(self.duration * self.sampling_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def _crop_or_pad(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ensure fixed length target_len via crop/pad. Crop start is random if augment=True.\"\"\"\n",
    "        if len(x) > self.maxlen:\n",
    "            x = x[:self.maxlen]\n",
    "\n",
    "        L = len(x)\n",
    "        if L > self.target_len:\n",
    "            if self.augment:\n",
    "                start = torch.randint(0, L - self.target_len + 1, (1,)).item()\n",
    "            else:\n",
    "                start = (L - self.target_len) // 2\n",
    "            x = x[start:start + self.target_len]\n",
    "        elif L < self.target_len:\n",
    "            pad = self.target_len - L\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _augment_audio(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Augment with:\n",
    "        - pitch shifting\n",
    "        - time stretching\n",
    "        - loudness (gain)\n",
    "        - additive noise\n",
    "        \"\"\"\n",
    "        x_np = x.numpy().astype(np.float32)\n",
    "\n",
    "        # Pitch shift (random 2 semitones)\n",
    "        if np.random.rand() < 0.7:\n",
    "            n_steps = np.random.uniform(-2.0, 2.0)\n",
    "            x_np = librosa.effects.pitch_shift(x_np, sr=self.sampling_rate, n_steps=n_steps)\n",
    "\n",
    "        # Time stretch (0.8x–1.2x)\n",
    "        if np.random.rand() < 0.7 and len(x_np) > 2:\n",
    "            rate = np.random.uniform(0.8, 1.2)\n",
    "            x_np = librosa.effects.time_stretch(x_np, rate)\n",
    "\n",
    "        x = torch.from_numpy(x_np)\n",
    "\n",
    "        # Loudness / gain\n",
    "        if torch.rand(1).item() < 0.7:\n",
    "            gain = 0.7 + 0.6 * torch.rand(1).item()  # 0.7x–1.3x\n",
    "            x = x * gain\n",
    "\n",
    "        # Additive Gaussian noise\n",
    "        if torch.rand(1).item() < 0.7:\n",
    "            std = x.std()\n",
    "            if std > 0:\n",
    "                noise_level = 0.02 * std   \n",
    "                noise = torch.randn_like(x) * noise_level\n",
    "                x = x + noise\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        audio_file = self.audio_files[idx]\n",
    "\n",
    "        genre_dir = audio_file[:audio_file.index('.')]\n",
    "        file_path = os.path.join(self.audio_path, genre_dir, audio_file)\n",
    "\n",
    "        rate, audio_samples = wav.read(file_path)\n",
    "\n",
    "        if audio_samples.ndim == 2:\n",
    "            audio_samples = audio_samples.mean(axis=1)\n",
    "\n",
    "        audio_samples = torch.from_numpy(audio_samples).to(torch.float32)\n",
    "\n",
    "        # Apply augmentation only to training data\n",
    "        if self.augment:\n",
    "            audio_samples = self._augment_audio(audio_samples)\n",
    "\n",
    "        # Fix length after augmentation\n",
    "        audio_samples = self._crop_or_pad(audio_samples)\n",
    "        audio_samples = audio_samples.unsqueeze(0)\n",
    "\n",
    "        return audio_samples, label\n",
    "\n",
    "label_map={'blues' : 0, 'classical' : 1, 'country' : 2,\n",
    "           'disco' : 3, 'hiphop'    : 4, 'jazz'    : 5,\n",
    "           'metal' : 6, 'pop'       : 7, 'reggae'  : 8, 'rock' : 9}\n",
    "\n",
    "audio_files = []\n",
    "labels = []\n",
    "for root, subdirs, files in os.walk(audio_dir):\n",
    "    for fname in files:\n",
    "        if fname == '.DS_Store':\n",
    "            continue\n",
    "        audio_files.append(fname)\n",
    "        labels.append(label_map[fname[:fname.index('.')]])\n",
    "\n",
    "torch.manual_seed(0) # Reproducible results\n",
    "\n",
    "# Create train/validation/test splits\n",
    "audio_length = len(audio_files)\n",
    "indices = torch.randperm(audio_length)\n",
    "\n",
    "train_size = int(0.7 * audio_length)\n",
    "val_size = int(0.15 * audio_length)\n",
    "test_size = audio_length - train_size - val_size\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_files = [audio_files[i] for i in train_indices]\n",
    "train_labels = [labels[i] for i in train_indices]\n",
    "validation_files = [audio_files[i] for i in val_indices]\n",
    "validation_labels = [labels[i] for i in val_indices]\n",
    "test_files = [audio_files[i] for i in test_indices]\n",
    "test_labels = [labels[i] for i in test_indices]\n",
    "\n",
    "print(\"Training set:\", len(train_files))\n",
    "print(\"Validation set:\", len(validation_files))\n",
    "print(\"Test set:\", len(test_files))\n",
    "assert len(set(train_files) & set(validation_files) & set(test_files)) == 0\n",
    "\n",
    "train_dataset = AudioDataset(audio_files=train_files, labels=train_labels,\n",
    "                             audio_path=audio_dir, \n",
    "                             maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "test_dataset=AudioDataset(audio_files=test_files, labels=test_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "val_dataset=AudioDataset(audio_files=validation_files, labels=validation_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False )\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "tmp_features, tmp_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # Mac MPS framework\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5882a",
   "metadata": {},
   "source": [
    "### A 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb460a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters are \n",
    "# cout try 2^4, 2^5 etc.\n",
    "# k kernel size \n",
    "#fdim is the size of the output of the last max pool layer (its value depends on settings for the other hyper-parameters)\n",
    "#M is the dimension of the linear layer prior to the final classification layer (try e.g., values the range 100 - 400)\n",
    "#r is repeats of conblock around 5 - 10 beware of overfitting\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device), target.to(device) # Move data to\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(data_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "def calculateFdim(L,k,r,stride):\n",
    "    for _ in range(r):\n",
    "        L=((L-k)//stride)+1\n",
    "        L=((L-k)//stride)+1\n",
    "    return L\n",
    "\n",
    "    \n",
    "\n",
    "class convBlock(torch.nn.Module):\n",
    "    def __init__(self,cin,cout,k):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(cin,cout,kernel_size=k,stride=2)\n",
    "        #self.conv2=nn.Conv1d(cin,cout,k,stride=2) extra conv1d\n",
    "        self.pool = nn.MaxPool1d(kernel_size=k, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=nn.functional.relu(self.conv1(x))\n",
    "        #x=nn.functional.relu(self.conv2(x))\n",
    "        x=self.pool(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class oneDCNN(torch.nn.Module):\n",
    "    def __init__(self,cin,cout,k,fdim,M,r):\n",
    "        super().__init__()\n",
    "        self.convBlock = nn.Sequential(*[convBlock(cin if i == 0 else cout, cout, k) for i in range(r)])\n",
    "        self.fc1=nn.Linear(fdim,M)\n",
    "        self.fc2=nn.Linear(M,10)\n",
    "\n",
    "\n",
    "    def forward(self, x):    \n",
    "        x=self.convBlock(x)\n",
    "        x=torch.flatten(x,start_dim=1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "cin=1\n",
    "cout=32\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "r = 5\n",
    "M=400\n",
    "n_epochs=50\n",
    "lam=0.1\n",
    "\n",
    "fdim=calculateFdim(22050,kernel_size,r,stride)*cout\n",
    "\n",
    "model=oneDCNN(cin,cout,kernel_size,fdim,M,r)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=lam)\n",
    "\n",
    "val_history=[]\n",
    "loss_history=[]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.functional.cross_entropy(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch: {epoch + 1:d}. Loss: {loss.item():.4f}, val_acc={val_acc}')\n",
    "    val_history.append(val_acc)\n",
    "    loss_history.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a307b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f'Test set accuracy: {test_acc:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
