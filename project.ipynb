{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa22828",
   "metadata": {},
   "source": [
    "### REI505M Final project: Music genre classification starter pack\n",
    "\n",
    "The following Dataset class operates on the GTZAN dataset.\n",
    "\n",
    "* The duration of most GTZAN files are 30 seconds (3022050=661500 samples) but some are slightly shorter (approx 29.9 seconds). For this reason we truncate at 660000 samples below.\n",
    "* It may be beneficial to work with smaller chunks than ~30 seconds.\n",
    "* You may want to perform the data augmentations in the `__get_item__` function.\n",
    "* For now, `train_dataset` contains all the dataset, you need to set aside some examples for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "audio_dir = 'music/' # Path to folder with GTZAN files\n",
    "# music/\n",
    "#  - rock/\n",
    "#       rock.00099.wav\n",
    "#       ...\n",
    "#  - reggie/\n",
    "#  ...\n",
    "#  - blues/\n",
    "\n",
    "batch_size = 32\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, audio_path,\n",
    "                 maxlen, sampling_rate, duration, augment=False):\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_path = audio_path\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.duration = duration          # seconds\n",
    "        self.augment = augment\n",
    "\n",
    "        self.target_len = int(self.duration * self.sampling_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def _crop_or_pad(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ensure fixed length target_len via crop/pad. Crop start is random if augment=True.\"\"\"\n",
    "        if len(x) > self.maxlen:\n",
    "            x = x[:self.maxlen]\n",
    "\n",
    "        L = len(x)\n",
    "        if L > self.target_len:\n",
    "            if self.augment:\n",
    "                start = torch.randint(0, L - self.target_len + 1, (1,)).item()\n",
    "            else:\n",
    "                start = (L - self.target_len) // 2\n",
    "            x = x[start:start + self.target_len]\n",
    "        elif L < self.target_len:\n",
    "            pad = self.target_len - L\n",
    "            x = torch.cat([x, torch.zeros(pad, dtype=x.dtype)], dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _augment_audio(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Augment with:\n",
    "        - pitch shifting\n",
    "        - time stretching\n",
    "        - loudness (gain)\n",
    "        - additive noise\n",
    "        \"\"\"\n",
    "        x_np = x.numpy().astype(np.float32)\n",
    "\n",
    "        # Pitch shift (random 2 semitones)\n",
    "        if np.random.rand() < 0.7:\n",
    "            n_steps = np.random.uniform(-2.0, 2.0)\n",
    "            x_np = librosa.effects.pitch_shift(x_np, sr=self.sampling_rate, n_steps=n_steps)\n",
    "\n",
    "        # Time stretch (0.8x–1.2x)\n",
    "        if np.random.rand() < 0.7 and len(x_np) > 2:\n",
    "            rate = np.random.uniform(0.8, 1.2)\n",
    "            x_np = librosa.effects.time_stretch(x_np, rate)\n",
    "\n",
    "        x = torch.from_numpy(x_np)\n",
    "\n",
    "        # Loudness / gain\n",
    "        if torch.rand(1).item() < 0.7:\n",
    "            gain = 0.7 + 0.6 * torch.rand(1).item()  # 0.7x–1.3x\n",
    "            x = x * gain\n",
    "\n",
    "        # Additive Gaussian noise\n",
    "        if torch.rand(1).item() < 0.7:\n",
    "            std = x.std()\n",
    "            if std > 0:\n",
    "                noise_level = 0.02 * std   \n",
    "                noise = torch.randn_like(x) * noise_level\n",
    "                x = x + noise\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        audio_file = self.audio_files[idx]\n",
    "\n",
    "        genre_dir = audio_file[:audio_file.index('.')]\n",
    "        file_path = os.path.join(self.audio_path, genre_dir, audio_file)\n",
    "\n",
    "        rate, audio_samples = wav.read(file_path)\n",
    "\n",
    "        if audio_samples.ndim == 2:\n",
    "            audio_samples = audio_samples.mean(axis=1)\n",
    "\n",
    "        audio_samples = torch.from_numpy(audio_samples).to(torch.float32)\n",
    "\n",
    "        # Apply augmentation only to training data\n",
    "        if self.augment:\n",
    "            audio_samples = self._augment_audio(audio_samples)\n",
    "\n",
    "        # Fix length after augmentation\n",
    "        audio_samples = self._crop_or_pad(audio_samples)\n",
    "        audio_samples = audio_samples.unsqueeze(0)\n",
    "\n",
    "        return audio_samples, label\n",
    "\n",
    "label_map={'blues' : 0, 'classical' : 1, 'country' : 2,\n",
    "           'disco' : 3, 'hiphop'    : 4, 'jazz'    : 5,\n",
    "           'metal' : 6, 'pop'       : 7, 'reggae'  : 8, 'rock' : 9}\n",
    "\n",
    "audio_files = []\n",
    "labels = []\n",
    "for root, subdirs, files in os.walk(audio_dir):\n",
    "    for fname in files:\n",
    "        if fname == '.DS_Store':\n",
    "            continue\n",
    "        audio_files.append(fname)\n",
    "        labels.append(label_map[fname[:fname.index('.')]])\n",
    "\n",
    "torch.manual_seed(0) # Reproducible results\n",
    "\n",
    "# Create train/validation/test splits\n",
    "audio_length = len(audio_files)\n",
    "indices = torch.randperm(audio_length)\n",
    "\n",
    "train_size = int(0.7 * audio_length)\n",
    "val_size = int(0.15 * audio_length)\n",
    "test_size = audio_length - train_size - val_size\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_files = [audio_files[i] for i in train_indices]\n",
    "train_labels = [labels[i] for i in train_indices]\n",
    "validation_files = [audio_files[i] for i in val_indices]\n",
    "validation_labels = [labels[i] for i in val_indices]\n",
    "test_files = [audio_files[i] for i in test_indices]\n",
    "test_labels = [labels[i] for i in test_indices]\n",
    "\n",
    "print(\"Training set:\", len(train_files))\n",
    "print(\"Validation set:\", len(validation_files))\n",
    "print(\"Test set:\", len(test_files))\n",
    "assert len(set(train_files) & set(validation_files) & set(test_files)) == 0\n",
    "\n",
    "train_dataset = AudioDataset(audio_files=train_files, labels=train_labels,\n",
    "                             audio_path=audio_dir, \n",
    "                             maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "test_dataset=AudioDataset(audio_files=test_files, labels=test_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "val_dataset=AudioDataset(audio_files=validation_files, labels=validation_labels,\n",
    "                          audio_path=audio_dir,\n",
    "                          maxlen=660000, sampling_rate=22050, duration=25, augment=True,)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False )\n",
    "val_loader = DataLoader(val_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "tmp_features, tmp_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5882a",
   "metadata": {},
   "source": [
    "### A 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb460a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters are \n",
    "# cout try 2^4, 2^5 etc.\n",
    "# k kernel size \n",
    "#fdim is the size of the output of the last max pool layer (its value depends on settings for the other hyper-parameters)\n",
    "#M is the dimension of the linear layer prior to the final classification layer (try e.g., values the range 100 - 400)\n",
    "#r is repeats of conblock around 5 - 10 beware of overfitting\n",
    "\n",
    "class convBlock(torch.nn.Module):\n",
    "    def __init__(self,cin,cout,k):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv1d(cin,cout,kernel_size=k,stride=2)\n",
    "        #self.conv2=nn.Conv1d(cout,k,stride=2) extra conv1d\n",
    "        self.pool = nn.MaxPool1d(kernel_size=k, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=nn.functional.relu(self.conv1(x))\n",
    "        #x=nn.functional.relu(self.conv2(x))\n",
    "        x=self.pool(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class oneDCNN(torch.nn.Module):\n",
    "    def __init__(self,cin,cout,k,fdim,M,r):\n",
    "        super().__init__()\n",
    "        self.convBlock=nn.RepeatLayer(convBlock(cin, cout, k), r)\n",
    "        self.fc1=nn.Linear(fdim,M)\n",
    "        self.fc2=nn.Linear(M,10)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):    \n",
    "        x=self.convBlock(x)\n",
    "        x=torch.flatten(x,start_dim=1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.softmax(x,dim=1)\n",
    "    \n",
    "\n",
    "cin=22050 \n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "r = 5\n",
    "\n",
    "#Calculate fdim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
